# ======================================================================================================================
# Application
# ======================================================================================================================
quarkus.application.name=s3-connector
quarkus.application.version=1.0.0
# Container Image Configuration
quarkus.container-image.registry=ghcr.io
quarkus.container-image.group=ai-pipestream
quarkus.container-image.name=s3-connector
quarkus.container-image.tag=latest
quarkus.container-image.additional-tags=${quarkus.application.version}
# ======================================================================================================================
# HTTP Configuration
# ======================================================================================================================
# No HTTP endpoints - connector doesn't expose REST API
quarkus.http.host=0.0.0.0
quarkus.http.port=38120
# Dev configuration
%dev.quarkus.http.port=38120
%dev.quarkus.http.host=0.0.0.0
# Test configuration
%test.quarkus.http.test-port=0
# ======================================================================================================================
# Dev Services Configuration
# ======================================================================================================================
# General Dev Services
%dev.quarkus.devservices.enabled=true
# Docker Compose Dev Services (Dev Profile) - Use shared dev services via extension
%dev.quarkus.compose.devservices.enabled=true
%dev.quarkus.compose.devservices.files=${user.home}/.pipeline/compose-devservices.yml
%dev.quarkus.devservices.timeout=120s
%dev.quarkus.compose.devservices.start-services=true
%dev.quarkus.compose.devservices.stop-services=false
%dev.quarkus.compose.devservices.reuse-project-for-tests=true
%dev.quarkus.compose.devservices.stop-timeout=30s
%dev.quarkus.compose.devservices.project-name=pipeline-shared-devservices
# Test profile - use compose devservices with container hostnames
%test.quarkus.devservices.enabled=true
%test.quarkus.compose.devservices.files=src/test/resources/compose-test-services.yml
%test.quarkus.compose.devservices.start-services=true
%test.quarkus.compose.devservices.project-name=pipeline-test-services
%test.quarkus.compose.devservices.stop-services=false
%test.quarkus.compose.devservices.reuse-project-for-tests=true
%test.quarkus.compose.devservices.stop-timeout=30s
%test.quarkus.compose.devservices.enabled=true
# Test Kafka config
%test.kafka.bootstrap.servers=${TEST_KAFKA_HOST:localhost}:${TEST_KAFKA_PORT:9095}
# ======================================================================================================================
# Service Registration Configuration
# ======================================================================================================================
pipestream.registration.enabled=true
pipestream.registration.service-name=s3-connector
pipestream.registration.description=S3 connector service for crawling S3 buckets and processing objects
pipestream.registration.type=CONNECTOR
# Advertised host: what clients use to connect
pipestream.registration.advertised-host=${SERVICE_REGISTRATION_ADVERTISED_HOST:host.docker.internal}
pipestream.registration.advertised-port=${quarkus.http.port}
# Internal host: what Consul uses for health checks
pipestream.registration.internal-host=${SERVICE_REGISTRATION_INTERNAL_HOST:172.17.0.1}
pipestream.registration.capabilities=s3-crawling,bucket-scanning
pipestream.registration.tags=s3,connector,storage
%test.pipestream.registration.enabled=false
# Registration service connection
pipestream.registration.registration-service.host=localhost
pipestream.registration.registration-service.port=38101
# ======================================================================================================================
# Kafka Configuration
# ======================================================================================================================
kafka.bootstrap.servers=${KAFKA_BOOTSTRAP_SERVERS:localhost:9094}
%dev.kafka.bootstrap.servers=localhost:9094
%prod.kafka.bootstrap.servers=kafka:9092
# Kafka Dev Services
%dev.quarkus.kafka.devservices.enabled=true
%prod.mp.messaging.connector.smallrye-kafka.bootstrap.servers=${kafka.bootstrap.servers}
%prod.mp.messaging.connector.smallrye-kafka.apicurio.registry.url=http://apicurio-registry:8080/apis/registry/v3
# ======================================================================================================================
# Kafka Channels
# ======================================================================================================================
# S3 crawl events channel (for listing objects during initial/recrawl)
# Note: Protobuf serialization is AUTO-CONFIGURED by the quarkus-apicurio-registry-protobuf extension
mp.messaging.outgoing.s3-crawl-events.topic=s3-crawl-events
# Incoming channel to process crawl events
mp.messaging.incoming.s3-crawl-events.topic=s3-crawl-events
mp.messaging.incoming.s3-crawl-events.failure-strategy=dead-letter-queue
# ======================================================================================================================
# AWS S3 Configuration
# ======================================================================================================================
# S3 endpoint (use MinIO endpoint for development, AWS S3 for production)
quarkus.s3.endpoint-override=${S3_ENDPOINT_OVERRIDE:}
%dev.quarkus.s3.endpoint-override=http://localhost:9000
%test.quarkus.s3.endpoint-override=http://localhost:9000
quarkus.s3.aws.region=${AWS_REGION:us-east-1}
%dev.quarkus.s3.aws.region=us-east-1
# AWS credentials (for MinIO, use minioadmin/minioadmin)
quarkus.s3.aws.credentials.type=static
quarkus.s3.aws.credentials.static-provider.access-key-id=${AWS_ACCESS_KEY_ID:minioadmin}
quarkus.s3.aws.credentials.static-provider.secret-access-key=${AWS_SECRET_ACCESS_KEY:minioadmin}
# Path-style access for MinIO
quarkus.s3.path-style-access=${S3_PATH_STYLE_ACCESS:true}
%dev.quarkus.s3.path-style-access=true
# ======================================================================================================================
# Connector Intake Service Configuration
# ======================================================================================================================
s3.connector.intake.base-url=${CONNECTOR_INTAKE_BASE_URL:http://localhost:38103}
s3.connector.intake.raw-path=/uploads/raw
s3.connector.intake.request-timeout=PT5M
# ======================================================================================================================
# S3 Connector Configuration
# ======================================================================================================================
# Default bucket for testing (can be overridden per datasource config)
s3.connector.default-bucket=${S3_DEFAULT_BUCKET:test-bucket}
# Crawl mode: initial-crawl, event-driven, or both
s3.connector.crawl-mode=${S3_CRAWL_MODE:initial-crawl}
# Initial crawl configuration
s3.connector.initial-crawl.enabled=true
s3.connector.initial-crawl.prefix=${S3_CRAWL_PREFIX:}
s3.connector.initial-crawl.max-keys-per-request=1000
# Event-driven mode (S3 notifications) - to be implemented later
s3.connector.event-driven.enabled=false
# ======================================================================================================================
# Apicurio Registry Configuration
# ======================================================================================================================
quarkus.index-dependency.pipestream-registration.group-id=ai.pipestream
quarkus.index-dependency.pipestream-registration.artifact-id=pipestream-service-registration
